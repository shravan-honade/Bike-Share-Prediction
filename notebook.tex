
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Your\_first\_neural\_network}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Your first neural network}\label{your-first-neural-network}

In this project, you'll build your first neural network and use it to
predict daily bike rental ridership. We've provided some of the code,
but left the implementation of the neural network up to you (for the
most part). After you've submitted this project, feel free to explore
the data and the model more.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        \PY{o}{\PYZpc{}}\PY{k}{config} InlineBackend.figure\PYZus{}format = \PYZsq{}retina\PYZsq{}
        
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\end{Verbatim}


    \subsection{Load and prepare the data}\label{load-and-prepare-the-data}

A critical step in working with neural networks is preparing the data
correctly. Variables on different scales make it difficult for the
network to efficiently learn the correct weights. Below, we've written
the code to load and prepare the data. You'll learn more about this
soon!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{data\PYZus{}path} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Bike\PYZhy{}Sharing\PYZhy{}Dataset/hour.csv}\PY{l+s+s1}{\PYZsq{}}
        
        \PY{n}{rides} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{data\PYZus{}path}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{rides}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
        \PY{n}{rides}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3}]:} (17379, 17)
\end{Verbatim}
            
    \subsection{Checking out the data}\label{checking-out-the-data}

This dataset has the number of riders for each hour of each day from
January 1 2011 to December 31 2012. The number of riders is split
between casual and registered, summed up in the \texttt{cnt} column. You
can see the first few rows of the data above.

Below is a plot showing the number of bike riders over the first 10 days
or so in the data set. (Some days don't have exactly 24 entries in the
data set, so it's not exactly 10 days.) You can see the hourly rentals
here. This data is pretty complicated! The weekends have lower over all
ridership and there are spikes when people are biking to and from work
during the week. Looking at the data above, we also have information
about temperature, humidity, and windspeed, all of these likely
affecting the number of riders. You'll be trying to capture all this
with your model.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{rides}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{24}\PY{o}{*}\PY{l+m+mi}{10}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dteday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x7f4c36d79dd8>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_6_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Dummy variables}\label{dummy-variables}

Here we have some categorical variables like season, weather, month. To
include these in our model, we'll need to make binary dummy variables.
This is simple to do with Pandas thanks to \texttt{get\_dummies()}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{dummy\PYZus{}fields} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{season}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weathersit}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hr}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weekday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{k}{for} \PY{n}{each} \PY{o+ow}{in} \PY{n}{dummy\PYZus{}fields}\PY{p}{:}
            \PY{n}{dummies} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{get\PYZus{}dummies}\PY{p}{(}\PY{n}{rides}\PY{p}{[}\PY{n}{each}\PY{p}{]}\PY{p}{,} \PY{n}{prefix}\PY{o}{=}\PY{n}{each}\PY{p}{,} \PY{n}{drop\PYZus{}first}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
            \PY{n}{rides} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{rides}\PY{p}{,} \PY{n}{dummies}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        
        \PY{n}{fields\PYZus{}to\PYZus{}drop} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{instant}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dteday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{season}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weathersit}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                          \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{weekday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{atemp}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mnth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{workingday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hr}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{data} \PY{o}{=} \PY{n}{rides}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{fields\PYZus{}to\PYZus{}drop}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:}    yr  holiday  temp   hum  windspeed  casual  registered  cnt  season\_1  \textbackslash{}
        0   0        0  0.24  0.81        0.0       3          13   16         1   
        1   0        0  0.22  0.80        0.0       8          32   40         1   
        2   0        0  0.22  0.80        0.0       5          27   32         1   
        3   0        0  0.24  0.75        0.0       3          10   13         1   
        4   0        0  0.24  0.75        0.0       0           1    1         1   
        
           season\_2    {\ldots}      hr\_21  hr\_22  hr\_23  weekday\_0  weekday\_1  weekday\_2  \textbackslash{}
        0         0    {\ldots}          0      0      0          0          0          0   
        1         0    {\ldots}          0      0      0          0          0          0   
        2         0    {\ldots}          0      0      0          0          0          0   
        3         0    {\ldots}          0      0      0          0          0          0   
        4         0    {\ldots}          0      0      0          0          0          0   
        
           weekday\_3  weekday\_4  weekday\_5  weekday\_6  
        0          0          0          0          1  
        1          0          0          0          1  
        2          0          0          0          1  
        3          0          0          0          1  
        4          0          0          0          1  
        
        [5 rows x 59 columns]
\end{Verbatim}
            
    \subsubsection{Scaling target variables}\label{scaling-target-variables}

To make training the network easier, we'll standardize each of the
continuous variables. That is, we'll shift and scale the variables such
that they have zero mean and a standard deviation of 1.

The scaling factors are saved so we can go backwards when we use the
network for predictions.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{quant\PYZus{}features} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{casual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{registered}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{temp}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{windspeed}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{c+c1}{\PYZsh{} Store scalings in a dictionary so we can convert back later}
        \PY{n}{scaled\PYZus{}features} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
        \PY{k}{for} \PY{n}{each} \PY{o+ow}{in} \PY{n}{quant\PYZus{}features}\PY{p}{:}
            \PY{n}{mean}\PY{p}{,} \PY{n}{std} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{each}\PY{p}{]}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{n}{each}\PY{p}{]}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{p}{)}
            \PY{n}{scaled\PYZus{}features}\PY{p}{[}\PY{n}{each}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{n}{mean}\PY{p}{,} \PY{n}{std}\PY{p}{]}
            \PY{n}{data}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{each}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{each}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{mean}\PY{p}{)}\PY{o}{/}\PY{n}{std}
\end{Verbatim}


    \subsubsection{Splitting the data into training, testing, and validation
sets}\label{splitting-the-data-into-training-testing-and-validation-sets}

We'll save the data for the last approximately 21 days to use as a test
set after we've trained the network. We'll use this set to make
predictions and compare them with the actual number of riders.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} Save data for approximately the last 21 days }
        \PY{n}{test\PYZus{}data} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{21}\PY{o}{*}\PY{l+m+mi}{24}\PY{p}{:}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} Now remove the test data from the data set }
        \PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{21}\PY{o}{*}\PY{l+m+mi}{24}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} Separate the data into features and targets}
        \PY{n}{target\PYZus{}fields} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{casual}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{registered}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{features}\PY{p}{,} \PY{n}{targets} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{target\PYZus{}fields}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{n}{target\PYZus{}fields}\PY{p}{]}
        \PY{n}{test\PYZus{}features}\PY{p}{,} \PY{n}{test\PYZus{}targets} \PY{o}{=} \PY{n}{test\PYZus{}data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{target\PYZus{}fields}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{test\PYZus{}data}\PY{p}{[}\PY{n}{target\PYZus{}fields}\PY{p}{]}
\end{Verbatim}


    We'll split the data into two sets, one for training and one for
validating as the network is being trained. Since this is time series
data, we'll train on historical data, then try to predict on future data
(the validation set).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{} Hold out the last 60 days or so of the remaining data as a validation set}
        \PY{n}{train\PYZus{}features}\PY{p}{,} \PY{n}{train\PYZus{}targets} \PY{o}{=} \PY{n}{features}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{60}\PY{o}{*}\PY{l+m+mi}{24}\PY{p}{]}\PY{p}{,} \PY{n}{targets}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{60}\PY{o}{*}\PY{l+m+mi}{24}\PY{p}{]}
        \PY{n}{val\PYZus{}features}\PY{p}{,} \PY{n}{val\PYZus{}targets} \PY{o}{=} \PY{n}{features}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{60}\PY{o}{*}\PY{l+m+mi}{24}\PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{targets}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{60}\PY{o}{*}\PY{l+m+mi}{24}\PY{p}{:}\PY{p}{]}
\end{Verbatim}


    \subsection{Time to build the network}\label{time-to-build-the-network}

Below you'll build your network. We've built out the structure. You'll
implement both the forward pass and backwards pass through the network.
You'll also set the hyperparameters: the learning rate, the number of
hidden units, and the number of training passes.

The network has two layers, a hidden layer and an output layer. The
hidden layer will use the sigmoid function for activations. The output
layer has only one node and is used for the regression, the output of
the node is the same as the input of the node. That is, the activation
function is \(f(x)=x\). A function that takes the input signal and
generates an output signal, but takes into account the threshold, is
called an activation function. We work through each layer of our network
calculating the outputs for each neuron. All of the outputs from one
layer become inputs to the neurons on the next layer. This process is
called \emph{forward propagation}.

We use the weights to propagate signals forward from the input to the
output layers in a neural network. We use the weights to also propagate
error backwards from the output back into the network to update our
weights. This is called \emph{backpropagation}.

\begin{quote}
\textbf{Hint:} You'll need the derivative of the output activation
function (\(f(x) = x\)) for the backpropagation implementation. If you
aren't familiar with calculus, this function is equivalent to the
equation \(y = x\). What is the slope of that equation? That is the
derivative of \(f(x)\).
\end{quote}

Below, you have these tasks: 1. Implement the sigmoid function to use as
the activation function. Set \texttt{self.activation\_function} in
\texttt{\_\_init\_\_} to your sigmoid function. 2. Implement the forward
pass in the \texttt{train} method. 3. Implement the backpropagation
algorithm in the \texttt{train} method, including calculating the output
error. 4. Implement the forward pass in the \texttt{run} method.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
        \PY{c+c1}{\PYZsh{} In the my\PYZus{}answers.py file, fill out the TODO sections as specified}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
        
        \PY{k+kn}{from} \PY{n+nn}{my\PYZus{}answers} \PY{k}{import} \PY{n}{NeuralNetwork}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k}{def} \PY{n+nf}{MSE}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{Y}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{Y}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
\end{Verbatim}


    \subsection{Unit tests}\label{unit-tests}

Run these unit tests to check the correctness of your network
implementation. This will help you be sure your network was implemented
correctly befor you starting trying to train it. These tests must all be
successful to pass the project.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{import} \PY{n+nn}{unittest}
         
         \PY{n}{inputs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{]}\PY{p}{]}\PY{p}{)}
         \PY{n}{targets} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mf}{0.4}\PY{p}{]}\PY{p}{]}\PY{p}{)}
         \PY{n}{test\PYZus{}w\PYZus{}i\PYZus{}h} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.2}\PY{p}{]}\PY{p}{,}
                                \PY{p}{[}\PY{l+m+mf}{0.4}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{]}\PY{p}{,}
                                \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{]}\PY{p}{]}\PY{p}{)}
         \PY{n}{test\PYZus{}w\PYZus{}h\PYZus{}o} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mf}{0.3}\PY{p}{]}\PY{p}{,}
                                \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.1}\PY{p}{]}\PY{p}{]}\PY{p}{)}
         
         \PY{k}{class} \PY{n+nc}{TestMethods}\PY{p}{(}\PY{n}{unittest}\PY{o}{.}\PY{n}{TestCase}\PY{p}{)}\PY{p}{:}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{c+c1}{\PYZsh{} Unit tests for data loading}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{k}{def} \PY{n+nf}{test\PYZus{}data\PYZus{}path}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} Test that file path to dataset has been unaltered}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{assertTrue}\PY{p}{(}\PY{n}{data\PYZus{}path}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bike\PYZhy{}sharing\PYZhy{}dataset/hour.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 
             \PY{k}{def} \PY{n+nf}{test\PYZus{}data\PYZus{}loaded}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} Test that data frame loaded}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{assertTrue}\PY{p}{(}\PY{n+nb}{isinstance}\PY{p}{(}\PY{n}{rides}\PY{p}{,} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{)}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{c+c1}{\PYZsh{} Unit tests for network functionality}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
         
             \PY{k}{def} \PY{n+nf}{test\PYZus{}activation}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                 \PY{n}{network} \PY{o}{=} \PY{n}{NeuralNetwork}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} Test that the activation function is a sigmoid}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{assertTrue}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{all}\PY{p}{(}\PY{n}{network}\PY{o}{.}\PY{n}{activation\PYZus{}function}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{1}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{+}\PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
             \PY{k}{def} \PY{n+nf}{test\PYZus{}train}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} Test that weights are updated correctly on training}
                 \PY{n}{network} \PY{o}{=} \PY{n}{NeuralNetwork}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{)}
                 \PY{n}{network}\PY{o}{.}\PY{n}{weights\PYZus{}input\PYZus{}to\PYZus{}hidden} \PY{o}{=} \PY{n}{test\PYZus{}w\PYZus{}i\PYZus{}h}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
                 \PY{n}{network}\PY{o}{.}\PY{n}{weights\PYZus{}hidden\PYZus{}to\PYZus{}output} \PY{o}{=} \PY{n}{test\PYZus{}w\PYZus{}h\PYZus{}o}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
                 
                 \PY{n}{network}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{inputs}\PY{p}{,} \PY{n}{targets}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{assertTrue}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{network}\PY{o}{.}\PY{n}{weights\PYZus{}hidden\PYZus{}to\PYZus{}output}\PY{p}{,} 
                                             \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[} \PY{l+m+mf}{0.37275328}\PY{p}{]}\PY{p}{,} 
                                                       \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.03172939}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{assertTrue}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{network}\PY{o}{.}\PY{n}{weights\PYZus{}input\PYZus{}to\PYZus{}hidden}\PY{p}{,}
                                             \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[} \PY{l+m+mf}{0.10562014}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{0.20185996}\PY{p}{]}\PY{p}{,} 
                                                       \PY{p}{[}\PY{l+m+mf}{0.39775194}\PY{p}{,} \PY{l+m+mf}{0.50074398}\PY{p}{]}\PY{p}{,} 
                                                       \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.29887597}\PY{p}{,} \PY{l+m+mf}{0.19962801}\PY{p}{]}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
             \PY{k}{def} \PY{n+nf}{test\PYZus{}run}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} Test correctness of run method}
                 \PY{n}{network} \PY{o}{=} \PY{n}{NeuralNetwork}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{)}
                 \PY{n}{network}\PY{o}{.}\PY{n}{weights\PYZus{}input\PYZus{}to\PYZus{}hidden} \PY{o}{=} \PY{n}{test\PYZus{}w\PYZus{}i\PYZus{}h}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
                 \PY{n}{network}\PY{o}{.}\PY{n}{weights\PYZus{}hidden\PYZus{}to\PYZus{}output} \PY{o}{=} \PY{n}{test\PYZus{}w\PYZus{}h\PYZus{}o}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
         
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{assertTrue}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{allclose}\PY{p}{(}\PY{n}{network}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{inputs}\PY{p}{)}\PY{p}{,} \PY{l+m+mf}{0.09998924}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{suite} \PY{o}{=} \PY{n}{unittest}\PY{o}{.}\PY{n}{TestLoader}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{loadTestsFromModule}\PY{p}{(}\PY{n}{TestMethods}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{unittest}\PY{o}{.}\PY{n}{TextTestRunner}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{suite}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\ldots}
----------------------------------------------------------------------
Ran 5 tests in 0.101s

OK

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}11}]:} <unittest.runner.TextTestResult run=5 errors=0 failures=0>
\end{Verbatim}
            
    \subsection{Training the network}\label{training-the-network}

Here you'll set the hyperparameters for the network. The strategy here
is to find hyperparameters such that the error on the training set is
low, but you're not overfitting to the data. If you train the network
too long or have too many hidden nodes, it can become overly specific to
the training set and will fail to generalize to the validation set. That
is, the loss on the validation set will start increasing as the training
set loss drops.

You'll also be using a method know as Stochastic Gradient Descent (SGD)
to train the network. The idea is that for each training pass, you grab
a random sample of the data instead of using the whole data set. You use
many more training passes than with normal gradient descent, but each
pass is much faster. This ends up training the network more efficiently.
You'll learn more about SGD later.

\subsubsection{Choose the number of
iterations}\label{choose-the-number-of-iterations}

This is the number of batches of samples from the training data we'll
use to train the network. The more iterations you use, the better the
model will fit the data. However, this process can have sharply
diminishing returns and can waste computational resources if you use too
many iterations. You want to find a number here where the network has a
low training loss, and the validation loss is at a minimum. The ideal
number of iterations would be a level that stops shortly after the
validation loss is no longer decreasing.

\subsubsection{Choose the learning rate}\label{choose-the-learning-rate}

This scales the size of weight updates. If this is too big, the weights
tend to explode and the network fails to fit the data. Normally a good
choice to start at is 0.1; however, if you effectively divide the
learning rate by n\_records, try starting out with a learning rate of 1.
In either case, if the network has problems fitting the data, try
reducing the learning rate. Note that the lower the learning rate, the
smaller the steps are in the weight updates and the longer it takes for
the neural network to converge.

\subsubsection{Choose the number of hidden
nodes}\label{choose-the-number-of-hidden-nodes}

In a model where all the weights are optimized, the more hidden nodes
you have, the more accurate the predictions of the model will be. (A
fully optimized model could have weights of zero, after all.) However,
the more hidden nodes you have, the harder it will be to optimize the
weights of the model, and the more likely it will be that suboptimal
weights will lead to overfitting. With overfitting, the model will
memorize the training data instead of learning the true pattern, and
won't generalize well to unseen data.

Try a few different numbers and see how it affects the performance. You
can look at the losses dictionary for a metric of the network
performance. If the number of hidden units is too low, then the model
won't have enough space to learn and if it is too high there are too
many options for the direction that the learning can take. The trick
here is to find the right balance in number of hidden units you choose.
You'll generally find that the best number of hidden nodes to use ends
up being between the number of input and output nodes.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{import} \PY{n+nn}{sys}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} Set the hyperparameters in you myanswers.py file \PYZsh{}\PYZsh{}\PYZsh{}}
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
         
         \PY{k+kn}{from} \PY{n+nn}{my\PYZus{}answers} \PY{k}{import} \PY{n}{iterations}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{hidden\PYZus{}nodes}\PY{p}{,} \PY{n}{output\PYZus{}nodes}
         
         
         \PY{n}{N\PYZus{}i} \PY{o}{=} \PY{n}{train\PYZus{}features}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
         \PY{n}{network} \PY{o}{=} \PY{n}{NeuralNetwork}\PY{p}{(}\PY{n}{N\PYZus{}i}\PY{p}{,} \PY{n}{hidden\PYZus{}nodes}\PY{p}{,} \PY{n}{output\PYZus{}nodes}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{)}
         
         \PY{n}{losses} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{validation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{p}{]}\PY{p}{\PYZcb{}}
         \PY{k}{for} \PY{n}{ii} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{iterations}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} Go through a random batch of 128 records from the training data set}
             \PY{n}{batch} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{train\PYZus{}features}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{128}\PY{p}{)}
             \PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{train\PYZus{}features}\PY{o}{.}\PY{n}{ix}\PY{p}{[}\PY{n}{batch}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{o}{.}\PY{n}{ix}\PY{p}{[}\PY{n}{batch}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
                                      
             \PY{n}{network}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Printing out the training progress}
             \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{n}{MSE}\PY{p}{(}\PY{n}{network}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{train\PYZus{}features}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{train\PYZus{}targets}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{)}
             \PY{n}{val\PYZus{}loss} \PY{o}{=} \PY{n}{MSE}\PY{p}{(}\PY{n}{network}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{val\PYZus{}features}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{,} \PY{n}{val\PYZus{}targets}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{)}
             \PY{n}{sys}\PY{o}{.}\PY{n}{stdout}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}r}\PY{l+s+s2}{Progress: }\PY{l+s+si}{\PYZob{}:2.1f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{l+m+mi}{100} \PY{o}{*} \PY{n}{ii}\PY{o}{/}\PY{n+nb}{float}\PY{p}{(}\PY{n}{iterations}\PY{p}{)}\PY{p}{)} \PYZbs{}
                              \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZpc{}}\PY{l+s+s2}{ ... Training loss: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{train\PYZus{}loss}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]} \PYZbs{}
                              \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ ... Validation loss: }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{val\PYZus{}loss}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
             \PY{n}{sys}\PY{o}{.}\PY{n}{stdout}\PY{o}{.}\PY{n}{flush}\PY{p}{(}\PY{p}{)}
             
             \PY{n}{losses}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{train\PYZus{}loss}\PY{p}{)}
             \PY{n}{losses}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{validation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{val\PYZus{}loss}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/shravan/anaconda3/envs/dlnd/lib/python3.6/site-packages/ipykernel\_launcher.py:17: DeprecationWarning: 
.ix is deprecated. Please use
.loc for label based indexing or
.iloc for positional indexing

See the documentation here:
http://pandas.pydata.org/pandas-docs/stable/indexing.html\#ix-indexer-is-deprecated

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Progress: 100.0\% {\ldots} Training loss: 0.066 {\ldots} Validation loss: 0.161
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{losses}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{losses}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{validation}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Check out your
predictions}\label{check-out-your-predictions}

Here, use the test data to view how well your network is modeling the
data. If something is completely wrong here, make sure each step in your
network is implemented correctly.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{mean}\PY{p}{,} \PY{n}{std} \PY{o}{=} \PY{n}{scaled\PYZus{}features}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{predictions} \PY{o}{=} \PY{n}{network}\PY{o}{.}\PY{n}{run}\PY{p}{(}\PY{n}{test\PYZus{}features}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{o}{*}\PY{n}{std} \PY{o}{+} \PY{n}{mean}
         \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{predictions}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Prediction}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{(}\PY{n}{test\PYZus{}targets}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cnt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{*}\PY{n}{std} \PY{o}{+} \PY{n}{mean}\PY{p}{)}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{n}{right}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{predictions}\PY{p}{)}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{dates} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{to\PYZus{}datetime}\PY{p}{(}\PY{n}{rides}\PY{o}{.}\PY{n}{ix}\PY{p}{[}\PY{n}{test\PYZus{}data}\PY{o}{.}\PY{n}{index}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dteday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{dates} \PY{o}{=} \PY{n}{dates}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{d}\PY{p}{:} \PY{n}{d}\PY{o}{.}\PY{n}{strftime}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{b }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xticks}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{dates}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{12}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{24}\PY{p}{]}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xticklabels}\PY{p}{(}\PY{n}{dates}\PY{p}{[}\PY{l+m+mi}{12}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{24}\PY{p}{]}\PY{p}{,} \PY{n}{rotation}\PY{o}{=}\PY{l+m+mi}{45}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/home/shravan/anaconda3/envs/dlnd/lib/python3.6/site-packages/ipykernel\_launcher.py:10: DeprecationWarning: 
.ix is deprecated. Please use
.loc for label based indexing or
.iloc for positional indexing

See the documentation here:
http://pandas.pydata.org/pandas-docs/stable/indexing.html\#ix-indexer-is-deprecated
  \# Remove the CWD from sys.path while we load stuff.

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_24_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{OPTIONAL: Thinking about your results(this question will not
be evaluated in the
rubric).}\label{optional-thinking-about-your-resultsthis-question-will-not-be-evaluated-in-the-rubric.}

Answer these questions about your results. How well does the model
predict the data? Where does it fail? Why does it fail where it does?

\begin{quote}
\textbf{Note:} You can edit the text in this cell by double clicking on
it. When you want to render the text, press control + enter
\end{quote}

\paragraph{Your answer below}\label{your-answer-below}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
